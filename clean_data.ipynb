{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mittalakshay6/DeepToxic/blob/master/clean_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "gzhSUXRj6YdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "9d54c848-38a3-49a7-bf66-2d58aab094bb"
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "# Work around misordering of STREAM and STDIN in Jupyter.\n",
        "# https://github.com/jupyter/notebook/issues/3159\n",
        "prompt = !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass(prompt[0] + '\\n\\nEnter verification code: ')\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "print('Files in Drive:')\n",
        "!ls drive/\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "\n",
            "Enter verification code: ··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "Files in Drive:\n",
            " HackingBooks  'Mummy weds Papa'   Softwares  'Untitled document.odt'\n",
            " Movies         PersonalDocs\t   Study\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kzEIyXW9welJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "729941f8-08f0-4e9f-a676-cc728bbd1673"
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## import packages\n",
        "########################################\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import operator\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ebir9zfGwelu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = 'Dataset/'\n",
        "TRAIN_DATA_FILE=path + 'train.csv'\n",
        "TEST_DATA_FILE=path + 'test.csv'\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
        "test_df = pd.read_csv(TEST_DATA_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98Z1rtSuwel9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Load the cleaned words\n",
        "########################################\n",
        "\n",
        "cl_path = 'features/cleanwords.txt'\n",
        "clean_word_dict = {}\n",
        "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
        "    for line in cl:\n",
        "        line = line.strip('\\n')\n",
        "        typo, correct = line.split(',')\n",
        "        clean_word_dict[typo] = correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2YBQSaIewemH",
        "colab_type": "code",
        "colab": {},
        "outputId": "27cea208-e7be-4582-b5fe-8fbf76c09aa4"
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## process texts in datasets\n",
        "########################################\n",
        "print('Processing text dataset')\n",
        "# Regex to remove all Non-Alpha Numeric and space\n",
        "special_character_removal=re.compile(r'[^?!.,:a-z\\d ]',re.IGNORECASE)\n",
        "\n",
        "# regex to replace all numerics\n",
        "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
        "word_count_dict = defaultdict(int)\n",
        "toxic_dict = {}\n",
        "\n",
        "def clean_text(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    # dirty words\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
        "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
        "    \n",
        "    if clean_wiki_tokens:\n",
        "        # Drop the image\n",
        "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.jpg\", \" \", text)\n",
        "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.png\", \" \", text)\n",
        "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.gif\", \" \", text)\n",
        "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.bmp\", \" \", text)\n",
        "\n",
        "        # Drop css\n",
        "        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n",
        "        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n",
        "        \n",
        "        # Clean templates\n",
        "        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n",
        "        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)        \n",
        "        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n",
        "        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n",
        "        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n",
        "        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n",
        "        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n",
        "        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n",
        "    \n",
        "    for typo, correct in clean_word_dict.items():\n",
        "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
        "\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\?\", \" ? \", text)\n",
        "    text = re.sub(r\"\\!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\\"\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = replace_numbers.sub(' ', text)\n",
        "    #text = special_character_removal.sub('',text)\n",
        "\n",
        "    if count_null_words:\n",
        "        text = text.split()\n",
        "        for t in text:\n",
        "            word_count_dict[t] += 1\n",
        "        text = \" \".join(text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "\n",
        "    return (text)\n",
        "\n",
        "list_sentences_train = train_df[\"comment_text\"].fillna(\"no comment\").values\n",
        "list_sentences_test = test_df[\"comment_text\"].fillna(\"no comment\").values\n",
        "\n",
        "comments = [clean_text(text) for text in list_sentences_train]    \n",
        "test_comments=[clean_text(text) for text in list_sentences_test]\n",
        "\n",
        "print(\"Cleaned.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing text dataset\n",
            "Cleaned.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "euvQ-QNtwemZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df['comment_text'] = comments\n",
        "test_df['comment_text'] = test_comments\n",
        "train_df.to_csv('Dataset/cleaned_train.csv', index=False)\n",
        "test_df.to_csv('Dataset/cleaned_test.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}